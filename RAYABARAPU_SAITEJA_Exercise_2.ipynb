{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RST0310/INFO-5731/blob/main/RAYABARAPU_SAITEJA_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # write your answer here\n",
        "# Research Question: Is there an alternate method that can be proposed to replace or supplement the Duckworth-Lewis (D/L) method in the game of cricket?\n",
        "\n",
        "# The Duckworth-Lewis method is currently used in cricket to calculate target scores in rain-affected limited-overs matches. However, it has faced criticism for its complexity and occasional lack of accuracy. Exploring alternatives could lead to more accurate and transparent methods for determining target scores in such scenarios.\n",
        "\n",
        "# Data Needed for Analysis:\n",
        "# 1. Match Data:\n",
        "#    - Basic match information (venue, date, teams involved)\n",
        "#    - Details of the match format (limited-overs, T20, ODI)\n",
        "#    - Innings-wise scores (both runs scored and wickets fallen)\n",
        "#    - Weather conditions during the match (rainfall, humidity, visibility, etc.)\n",
        "#    - Duration and timing of rain interruptions, if any\n",
        "\n",
        "# 2. Historical Match Data:\n",
        "#    - A significant dataset of historical matches, including both rain-affected and non-rain-affected matches\n",
        "#    - Similar match data as described above, covering a diverse range of playing conditions and venues\n",
        "\n",
        "# 3. Outcome Data:\n",
        "#    - The actual match outcome (win, loss, tie) for each match\n",
        "#    - Comparison of match results using Duckworth-Lewis method vs. actual outcomes\n",
        "\n",
        "# 4. Potential Alternate Method Data:\n",
        "#    - Data related to any proposed alternate methods for calculating target scores\n",
        "#    - Simulation results or theoretical calculations based on these alternate methods\n",
        "\n",
        "# Amount of Data Needed:\n",
        "# - Match Data: Ideally, a dataset covering several seasons or years of cricket matches across different formats and conditions would be beneficial. This could include hundreds or even thousands of matches.\n",
        "# - Historical Match Data: Similar to match data, a large dataset covering a significant period of cricket history would be required.\n",
        "# - Outcome Data: Data on the outcomes of all matches in the dataset.\n",
        "# - Potential Alternate Method Data: Data related to proposed alternate methods and their simulations or theoretical calculations.\n",
        "\n",
        "# Steps for Collecting and Saving Data:\n",
        "# 1. Identify reliable sources for cricket match data, such as cricket databases, official cricket websites, or APIs provided by cricket organizations.\n",
        "# 2. Extract match data for the desired period, covering various formats and conditions. This may involve web scraping or using APIs to access the data.\n",
        "# 3. Organize the data into a structured format, including match details, innings data, weather information, and any other relevant variables.\n",
        "# 4. Collect historical match data using similar methods to ensure a comprehensive dataset.\n",
        "# 5. Compile outcome data by cross-referencing match results with the match data collected.\n",
        "# 6. Research and gather data on any proposed alternate methods for calculating target scores.\n",
        "# 7. Store the collected data in a secure and accessible database or spreadsheet format for analysis.\n",
        "\n",
        "# With this comprehensive dataset, researchers can analyze the performance of the Duckworth-Lewis method compared to actual match outcomes and explore potential alternate methods for calculating target scores in rain-affected cricket matches."
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Sample venues, teams, and outcomes\n",
        "venues = ['Eden Gardens', 'Lord\\'s Cricket Ground', 'Melbourne Cricket Ground', 'Sydney Cricket Ground', 'Wankhede Stadium']\n",
        "teams = ['India', 'Australia', 'England', 'Pakistan', 'South Africa']\n",
        "outcomes = ['win', 'loss', 'tie']\n",
        "\n",
        "# Function to generate random match data\n",
        "def generate_match_data():\n",
        "    venue = random.choice(venues)\n",
        "    date = datetime.now() - timedelta(days=random.randint(1, 365))  # Random date within the past year\n",
        "    teams_playing = random.sample(teams, 2)\n",
        "    format = random.choice(['T20', 'ODI'])\n",
        "    innings1_runs = random.randint(100, 400)\n",
        "    innings1_wickets = random.randint(0, 10)\n",
        "    innings2_runs = random.randint(50, innings1_runs)  # Ensure second innings score is lower than first\n",
        "    innings2_wickets = random.randint(0, 10)\n",
        "    rainfall = random.uniform(0, 50)  # Simulating rainfall in mm\n",
        "    humidity = random.uniform(0, 100)  # Simulating humidity percentage\n",
        "    visibility = random.uniform(0, 10)  # Simulating visibility in km\n",
        "    outcome = random.choice(outcomes)\n",
        "\n",
        "    return {\n",
        "        'venue': venue,\n",
        "        'date': date.strftime('%Y-%m-%d'),\n",
        "        'teams': teams_playing,\n",
        "        'format': format,\n",
        "        'innings1_runs': innings1_runs,\n",
        "        'innings1_wickets': innings1_wickets,\n",
        "        'innings2_runs': innings2_runs,\n",
        "        'innings2_wickets': innings2_wickets,\n",
        "        'rainfall': rainfall,\n",
        "        'humidity': humidity,\n",
        "        'visibility': visibility,\n",
        "        'outcome': outcome\n",
        "    }\n",
        "\n",
        "# Collect 1000 samples of match data\n",
        "num_samples = 1000\n",
        "dataset = [generate_match_data() for _ in range(num_samples)]\n",
        "\n",
        "# Convert dataset to DataFrame and save to CSV file\n",
        "df = pd.DataFrame(dataset)\n",
        "df.to_csv('cricket_match_dataset.csv', index=False)\n",
        "print(\"Dataset saved successfully.\")\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd33bb59-d2dd-41b3-dbbb-8da87b230366"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz7bQzA36o8G",
        "outputId": "5392043f-f9cc-4644-d9d5-19250b7c9c1c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.11.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGYP8hsh6u6Y",
        "outputId": "45ec3cb9-a85c-426e-cff2-0eb83a385e67"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import random\n",
        "from time import sleep\n",
        "\n",
        "def fetch_article_info(keyword, year_range, num_articles):\n",
        "    base_url = \"https://www.semanticscholar.org\"\n",
        "    articles_info = []\n",
        "\n",
        "    # Iterate through multiple pages of search results\n",
        "    for page_num in range(1, (num_articles // 10) + 2):\n",
        "        url = f\"https://www.semanticscholar.org/search?q={keyword}&sort=relevance&page={page_num}\"\n",
        "        response = urllib.request.urlopen(url)\n",
        "        soup = BeautifulSoup(response.read(), 'html.parser')\n",
        "\n",
        "        # Extract article links from the search page\n",
        "        article_links = soup.select(\"a[data-selenium-selector='title-link']\")\n",
        "        for link in article_links:\n",
        "            article_url = base_url + link['href']\n",
        "            article_info = extract_article_info(article_url)\n",
        "            if article_info:\n",
        "                articles_info.append(article_info)\n",
        "                if len(articles_info) >= num_articles:\n",
        "                    break\n",
        "        sleep(random.uniform(1, 3))  # Add some delay between requests to avoid being blocked\n",
        "\n",
        "    return articles_info\n",
        "\n",
        "def extract_article_info(article_url):\n",
        "    response = urllib.request.urlopen(article_url)\n",
        "    soup = BeautifulSoup(response.read(), 'html.parser')\n",
        "\n",
        "    # Extracting article details\n",
        "    title = soup.find(\"h1\", class_=\"heading\").text.strip() if soup.find(\"h1\", class_=\"heading\") else \"\"\n",
        "    venue = soup.find(\"span\", class_=\"venue\").text.strip() if soup.find(\"span\", class_=\"venue\") else \"\"\n",
        "    year = soup.find(\"span\", class_=\"citation__year\").text.strip() if soup.find(\"span\", class_=\"citation__year\") else \"\"\n",
        "    authors = [author.text.strip() for author in soup.select(\"span.author-list__name\")] if soup.select(\"span.author-list__name\") else []\n",
        "    abstract = soup.find(\"div\", class_=\"text-truncator abstract__text\").text.strip() if soup.find(\"div\", class_=\"text-truncator abstract__text\") else \"\"\n",
        "\n",
        "    article_info = {\n",
        "        'title': title,\n",
        "        'venue': venue,\n",
        "        'year': year,\n",
        "        'authors': authors,\n",
        "        'abstract': abstract\n",
        "    }\n",
        "\n",
        "    return article_info\n",
        "\n",
        "# Keyword, year range, and number of articles to fetch\n",
        "keyword = \"XYZ\"\n",
        "year_range = range(2014, 2025)\n",
        "num_articles = 1000\n",
        "\n",
        "# Fetch article information\n",
        "articles_info = fetch_article_info(keyword, year_range, num_articles)\n",
        "\n",
        "# Print number of articles collected\n",
        "print(f\"Number of articles collected: {len(articles_info)}\")\n",
        "\n",
        "# Print 5 samples if articles are available\n",
        "if len(articles_info) > 0:\n",
        "    print(\"\\nSample articles:\")\n",
        "    for i in range(min(5, len(articles_info))):\n",
        "        print(articles_info[i])\n",
        "else:\n",
        "    print(\"No articles found.\")\n",
        "\n",
        "# Convert to DataFrame and save to CSV\n",
        "df = pd.DataFrame(articles_info)\n",
        "df.to_csv('semantic_scholar_articles.csv', index=False)\n",
        "\n",
        "print(\"Articles collected and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSxYsOvqs-Qx",
        "outputId": "60581461-2e14-43f4-c82e-89cbd42527dd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of articles collected: 0\n",
            "No articles found.\n",
            "Articles collected and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a4c638-83c6-4663-991f-75914a7ecd1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 400 - {\"error\":{\"message\":\"(#100) Tried accessing nonexisting field (posts) on node type (Application)\",\"type\":\"OAuthException\",\"code\":100,\"fbtrace_id\":\"AyvEDvOw7y_zsUBjaP_WySb\"}}\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Update your Facebook App credentials\n",
        "app_id = \"325885207078723\"\n",
        "app_secret = \"83d30c2b9d413ae28f71e27d3baa8d20\"\n",
        "access_token = \"EAAEoZABABZA0MBO33Ew0xtgs08cTsRPbqYSZBXlxWyPf6WRRyg5M28ZA4hvXv8k5FRsc4vXwjaIXFCLwewHBC4B4J9CVVD6PP2MzmzEnBIFNVpKbhhnPIIZAiNdzDQcbgP5zzEDZBBqtVHyS5aCZCQvgD2eq29n4iaKKMSPAMgrvA7RhC7wbPgP08v0JKNZBlY3HpGL3doNAIU7JkF84t1WVUAb9aWEao6CqdLiQQblZAyWtAPzCy73vI3oZBde48J6hN7bGUnL06HZBu0ZD\"\n",
        "\n",
        "def get_facebook_page_posts(page_id, access_token, limit=100):\n",
        "    base_url = f\"https://graph.facebook.com/v12.0/{page_id}/posts\"\n",
        "    params = {\n",
        "        \"access_token\": access_token,\n",
        "        \"limit\": limit,\n",
        "        \"fields\": \"id,message,created_time,likes.summary(true),shares.summary(true),comments.summary(true)\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error: {response.status_code} - {response.text}\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        data = response.json()\n",
        "        if 'data' in data:\n",
        "            posts_data = []\n",
        "            for post in data['data']:\n",
        "                post_data = {\n",
        "                    'Post ID': post['id'],\n",
        "                    'Message': post.get('message', ''),\n",
        "                    'Created Time': post['created_time'],\n",
        "                    'Likes': post['likes']['summary']['total_count'],\n",
        "                    'Shares': post['shares']['count'] if 'shares' in post else 0,\n",
        "                    'Comments': post['comments']['summary']['total_count']\n",
        "                }\n",
        "                posts_data.append(post_data)\n",
        "            return posts_data\n",
        "        else:\n",
        "            print(\"No 'data' key found in the response.\")\n",
        "            return []\n",
        "    except json.decoder.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON: {e}\")\n",
        "        return []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    page_id = \"325885207078723\"  # Replace \"your_page_id\" with the actual ID of your Facebook page\n",
        "    limit = 100\n",
        "    posts_data = get_facebook_page_posts(page_id, access_token, limit)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    posts_df = pd.DataFrame(posts_data)\n",
        "    print(posts_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# This webscarping is quite challenging,and it takes a good skill to extract the data. I understood how to generate tokens and access them. I has to create a developer accound on meta and genereate the acess token which is quite exciting. It helped me with a lot of understanding of where to find the extract data required for the research.",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "55W9AMdXCSpV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
