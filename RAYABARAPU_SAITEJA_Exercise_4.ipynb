{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RST0310/INFO-5731/blob/main/RAYABARAPU_SAITEJA_Exercise_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 4**\n",
        "\n",
        "**This exercise will provide a valuable learning experience in working with text data and extracting features using various topic modeling algorithms. Key concepts such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "\n",
        "**Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0388d2f8-aee5-4a75-a89f-ade5b849a4b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import CoherenceModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"One interesting text classification task related to cricket could be classifying match commentary or articles into different categories such as match summaries, player profiles, match predictions, and analysis pieces. Here are some features that could be useful for building a machine learning model for this task:\",\n",
        "    \"1. Word Frequency Features: The frequency of specific cricket-related terms or phrases in the text could be indicative of its category. For example, terms like 'century,' 'wicket,' 'runs,' 'dismissal,' and 'boundary' might be more common in match summaries, while terms like 'player statistics,' 'performance analysis,' and 'strategy' might be more common in analysis pieces.\",\n",
        "    \"2. N-grams: Analyzing sequences of words (n-grams) could provide valuable context. For instance, phrases like 'man of the match,' 'caught behind,' or 'run rate' might be indicative of specific types of content.\",\n",
        "    \"3. Sentiment Analysis: The sentiment expressed in the text could help classify articles as positive (e.g., celebrating a team's victory), negative (e.g., criticizing a player's performance), or neutral. This could be achieved by analyzing the sentiment of individual sentences or using pre-trained sentiment analysis models.\",\n",
        "    \"4. Named Entity Recognition (NER): Identifying named entities such as player names, team names, tournament names, and locations mentioned in the text could provide clues about the content's category. For example, mentions of players and teams are likely to appear in player profiles or match summaries.\",\n",
        "    \"5. Part-of-Speech (POS) Tagging: Analyzing the grammatical structure of the text by tagging words with their parts of speech could reveal patterns specific to certain categories. For instance, articles with a high frequency of verbs related to analysis (e.g., 'analyze,' 'evaluate,' 'assess') might belong to the analysis category.\",\n",
        "    \"6. Topic Modeling: Identifying underlying topics in the text using techniques like Latent Dirichlet Allocation (LDA) or Non-Negative Matrix Factorization (NMF) could help categorize articles based on the dominant themes present. For instance, topics related to match tactics, player performances, or match outcomes might be indicative of specific categories.\",\n",
        "    \"These features capture different aspects of the text content, allowing the machine learning model to learn patterns associated with each category and improve classification accuracy. By combining multiple types of features, the model can better understand the usage of cricket-related text and accurately classify it into relevant categories.\"\n",
        "]\n",
        "\n",
        "# Preprocess the text data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "texts = [word_tokenize(document.lower()) for document in documents]\n",
        "texts = [[word for word in text if word.isalnum() and word not in stop_words] for text in texts]\n",
        "\n",
        "# Create a dictionary representation of the documents\n",
        "dictionary = Dictionary(texts)\n",
        "\n",
        "# Create a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Determine the optimal number of topics based on coherence score\n",
        "coherence_scores = []\n",
        "for k in range(2, 11):\n",
        "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, random_state=42)\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    coherence_scores.append((k, coherence_score))\n",
        "\n",
        "best_k, best_coherence = max(coherence_scores, key=lambda x: x[1])\n",
        "\n",
        "# Train the LDA model with the best number of topics\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=best_k, random_state=42)\n",
        "\n",
        "# Print the topics\n",
        "print(f\"The best number of topics (K) based on coherence score: {best_k}\")\n",
        "print(\"Summarized Topics:\")\n",
        "for idx, topic in lda_model.print_topics():\n",
        "    print(f'Topic {idx + 1}: {topic}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPdzP9jjaW-J",
        "outputId": "6fc7923a-9fa6-4bf9-fd98-ad6f5f2bfba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best number of topics (K) based on coherence score: 7\n",
            "Summarized Topics:\n",
            "Topic 1: 0.030*\"content\" + 0.029*\"indicative\" + 0.029*\"might\" + 0.028*\"words\" + 0.028*\"specific\" + 0.028*\"sequences\" + 0.028*\"instance\" + 0.028*\"context\" + 0.028*\"could\" + 0.028*\"types\"\n",
            "Topic 2: 0.046*\"tagging\" + 0.045*\"analysis\" + 0.025*\"articles\" + 0.025*\"might\" + 0.025*\"specific\" + 0.025*\"categories\" + 0.025*\"grammatical\" + 0.025*\"parts\" + 0.025*\"frequency\" + 0.025*\"words\"\n",
            "Topic 3: 0.032*\"sentiment\" + 0.028*\"match\" + 0.025*\"could\" + 0.024*\"topics\" + 0.022*\"using\" + 0.020*\"player\" + 0.020*\"text\" + 0.019*\"help\" + 0.019*\"articles\" + 0.017*\"analysis\"\n",
            "Topic 4: 0.008*\"match\" + 0.008*\"might\" + 0.008*\"could\" + 0.008*\"analysis\" + 0.008*\"text\" + 0.008*\"like\" + 0.008*\"player\" + 0.008*\"features\" + 0.008*\"specific\" + 0.008*\"sentiment\"\n",
            "Topic 5: 0.039*\"model\" + 0.037*\"features\" + 0.035*\"text\" + 0.028*\"match\" + 0.025*\"categories\" + 0.024*\"classification\" + 0.024*\"machine\" + 0.023*\"different\" + 0.022*\"learning\" + 0.021*\"types\"\n",
            "Topic 6: 0.043*\"could\" + 0.041*\"analysis\" + 0.035*\"match\" + 0.032*\"text\" + 0.031*\"sentiment\" + 0.025*\"terms\" + 0.022*\"like\" + 0.021*\"might\" + 0.021*\"player\" + 0.020*\"articles\"\n",
            "Topic 7: 0.049*\"names\" + 0.034*\"player\" + 0.033*\"named\" + 0.027*\"could\" + 0.026*\"text\" + 0.026*\"match\" + 0.025*\"category\" + 0.022*\"summaries\" + 0.021*\"example\" + 0.019*\"identifying\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "\n",
        "**Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fcda226-15aa-4759-f0c0-6bbd7e6198df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best number of topics (K) based on coherence-like score: 2\n",
            "Summarized Topics:\n",
            "Topic 1: match analysis could might like terms text specific frequency features\n",
            "Topic 2: model features task learning classification different machine improve combining capture\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import normalize\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"One interesting text classification task related to cricket could be classifying match commentary or articles into different categories such as match summaries, player profiles, match predictions, and analysis pieces. Here are some features that could be useful for building a machine learning model for this task:\",\n",
        "    \"1. Word Frequency Features: The frequency of specific cricket-related terms or phrases in the text could be indicative of its category. For example, terms like 'century,' 'wicket,' 'runs,' 'dismissal,' and 'boundary' might be more common in match summaries, while terms like 'player statistics,' 'performance analysis,' and 'strategy' might be more common in analysis pieces.\",\n",
        "    \"2. N-grams: Analyzing sequences of words (n-grams) could provide valuable context. For instance, phrases like 'man of the match,' 'caught behind,' or 'run rate' might be indicative of specific types of content.\",\n",
        "    \"3. Sentiment Analysis: The sentiment expressed in the text could help classify articles as positive (e.g., celebrating a team's victory), negative (e.g., criticizing a player's performance), or neutral. This could be achieved by analyzing the sentiment of individual sentences or using pre-trained sentiment analysis models.\",\n",
        "    \"4. Named Entity Recognition (NER): Identifying named entities such as player names, team names, tournament names, and locations mentioned in the text could provide clues about the content's category. For example, mentions of players and teams are likely to appear in player profiles or match summaries.\",\n",
        "    \"5. Part-of-Speech (POS) Tagging: Analyzing the grammatical structure of the text by tagging words with their parts of speech could reveal patterns specific to certain categories. For instance, articles with a high frequency of verbs related to analysis (e.g., 'analyze,' 'evaluate,' 'assess') might belong to the analysis category.\",\n",
        "    \"6. Topic Modeling: Identifying underlying topics in the text using techniques like Latent Dirichlet Allocation (LDA) or Non-Negative Matrix Factorization (NMF) could help categorize articles based on the dominant themes present. For instance, topics related to match tactics, player performances, or match outcomes might be indicative of specific categories.\",\n",
        "    \"These features capture different aspects of the text content, allowing the machine learning model to learn patterns associated with each category and improve classification accuracy. By combining multiple types of features, the model can better understand the usage of cricket-related text and accurately classify it into relevant categories.\"\n",
        "]\n",
        "\n",
        "# Preprocess the text data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "texts = [' '.join([word for word in word_tokenize(document.lower()) if word.isalnum() and word not in stop_words]) for document in documents]\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Compute the truncated SVD (LSA)\n",
        "num_topics_range = range(2, 11)\n",
        "coherence_scores = []\n",
        "\n",
        "for num_topics in num_topics_range:\n",
        "    svd = TruncatedSVD(n_components=num_topics)\n",
        "    document_topics = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "    # Compute coherence-like score\n",
        "    similarity_matrix = cosine_similarity(document_topics)\n",
        "    np.fill_diagonal(similarity_matrix, -1)  # Set diagonal to -1 to ignore self-similarity\n",
        "    coherence_score = np.mean(similarity_matrix)\n",
        "    coherence_scores.append((num_topics, coherence_score))\n",
        "\n",
        "# Choose the number of topics with the highest coherence-like score\n",
        "best_k, best_coherence = max(coherence_scores, key=lambda x: x[1])\n",
        "\n",
        "# Train LSA model with the best number of topics\n",
        "svd = TruncatedSVD(n_components=best_k)\n",
        "document_topics = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "# Normalize document-topic matrix\n",
        "document_topics = normalize(document_topics, axis=1, norm='l2')\n",
        "\n",
        "# Print the topics\n",
        "print(f\"The best number of topics (K) based on coherence-like score: {best_k}\")\n",
        "print(\"Summarized Topics:\")\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "for idx, topic in enumerate(svd.components_):\n",
        "    top_words_idx = topic.argsort()[-10:][::-1]  # Get the indices of top 10 words for each topic\n",
        "    top_words = [feature_names[i] for i in top_words_idx]\n",
        "    print(f\"Topic {idx + 1}: {' '.join(top_words)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "**Generate K topics by using lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d8401b3-d7a8-4ff7-f780-8de0c9d968bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best number of topics (K) based on coherence score: 8\n",
            "Summarized Topics:\n",
            "Topic 1: 0.044*\"analysis\" + 0.043*\"tagging\" + 0.027*\"speech\" + 0.027*\"analyzing\" + 0.026*\"structure\" + 0.026*\"grammatical\" + 0.026*\"categories\" + 0.026*\"high\" + 0.025*\"patterns\" + 0.025*\"parts\"\n",
            "Topic 2: 0.052*\"sentiment\" + 0.027*\"features\" + 0.027*\"analysis\" + 0.027*\"classify\" + 0.027*\"model\" + 0.019*\"articles\" + 0.018*\"player\" + 0.018*\"using\" + 0.018*\"help\" + 0.018*\"categories\"\n",
            "Topic 3: 0.036*\"tagging\" + 0.035*\"analysis\" + 0.024*\"pos\" + 0.024*\"reveal\" + 0.023*\"certain\" + 0.022*\"articles\" + 0.021*\"instance\" + 0.021*\"specific\" + 0.020*\"words\" + 0.020*\"might\"\n",
            "Topic 4: 0.034*\"topics\" + 0.021*\"categories\" + 0.021*\"lda\" + 0.021*\"factorization\" + 0.020*\"allocation\" + 0.020*\"related\" + 0.020*\"categorize\" + 0.020*\"outcomes\" + 0.020*\"performances\" + 0.020*\"help\"\n",
            "Topic 5: 0.053*\"task\" + 0.029*\"analysis\" + 0.029*\"features\" + 0.028*\"pieces\" + 0.028*\"summaries\" + 0.028*\"interesting\" + 0.028*\"profiles\" + 0.028*\"player\" + 0.028*\"related\" + 0.028*\"model\"\n",
            "Topic 6: 0.057*\"terms\" + 0.057*\"might\" + 0.056*\"like\" + 0.039*\"frequency\" + 0.039*\"common\" + 0.039*\"analysis\" + 0.039*\"specific\" + 0.039*\"phrases\" + 0.038*\"indicative\" + 0.021*\"category\"\n",
            "Topic 7: 0.033*\"topics\" + 0.020*\"tactics\" + 0.019*\"based\" + 0.019*\"articles\" + 0.019*\"using\" + 0.019*\"themes\" + 0.019*\"instance\" + 0.018*\"topic\" + 0.018*\"modeling\" + 0.018*\"player\"\n",
            "Topic 8: 0.072*\"names\" + 0.049*\"player\" + 0.049*\"named\" + 0.026*\"provide\" + 0.026*\"content\" + 0.026*\"mentioned\" + 0.026*\"team\" + 0.026*\"summaries\" + 0.026*\"example\" + 0.026*\"players\"\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import CoherenceModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"One interesting text classification task related to cricket could be classifying match commentary or articles into different categories such as match summaries, player profiles, match predictions, and analysis pieces. Here are some features that could be useful for building a machine learning model for this task:\",\n",
        "    \"1. Word Frequency Features: The frequency of specific cricket-related terms or phrases in the text could be indicative of its category. For example, terms like 'century,' 'wicket,' 'runs,' 'dismissal,' and 'boundary' might be more common in match summaries, while terms like 'player statistics,' 'performance analysis,' and 'strategy' might be more common in analysis pieces.\",\n",
        "    \"2. N-grams: Analyzing sequences of words (n-grams) could provide valuable context. For instance, phrases like 'man of the match,' 'caught behind,' or 'run rate' might be indicative of specific types of content.\",\n",
        "    \"3. Sentiment Analysis: The sentiment expressed in the text could help classify articles as positive (e.g., celebrating a team's victory), negative (e.g., criticizing a player's performance), or neutral. This could be achieved by analyzing the sentiment of individual sentences or using pre-trained sentiment analysis models.\",\n",
        "    \"4. Named Entity Recognition (NER): Identifying named entities such as player names, team names, tournament names, and locations mentioned in the text could provide clues about the content's category. For example, mentions of players and teams are likely to appear in player profiles or match summaries.\",\n",
        "    \"5. Part-of-Speech (POS) Tagging: Analyzing the grammatical structure of the text by tagging words with their parts of speech could reveal patterns specific to certain categories. For instance, articles with a high frequency of verbs related to analysis (e.g., 'analyze,' 'evaluate,' 'assess') might belong to the analysis category.\",\n",
        "    \"6. Topic Modeling: Identifying underlying topics in the text using techniques like Latent Dirichlet Allocation (LDA) or Non-Negative Matrix Factorization (NMF) could help categorize articles based on the dominant themes present. For instance, topics related to match tactics, player performances, or match outcomes might be indicative of specific categories.\",\n",
        "    \"These features capture different aspects of the text content, allowing the machine learning model to learn patterns associated with each category and improve classification accuracy. By combining multiple types of features, the model can better understand the usage of cricket-related text and accurately classify it into relevant categories.\"\n",
        "]\n",
        "\n",
        "# Preprocess the text data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "texts = [word_tokenize(document.lower()) for document in documents]\n",
        "texts = [[word for word in text if word.isalnum() and word not in stop_words] for text in texts]\n",
        "\n",
        "# Remove documents with no terms after preprocessing\n",
        "texts = [text for text in texts if text]\n",
        "\n",
        "# Create a dictionary representation of the documents\n",
        "dictionary = Dictionary(texts)\n",
        "\n",
        "# Filter out words that occur less than 1 document, or more than 50% of the documents\n",
        "dictionary.filter_extremes(no_below=1, no_above=0.5)\n",
        "\n",
        "# Create a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Determine the optimal number of topics\n",
        "coherence_scores = []\n",
        "for k in range(2, 11):\n",
        "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, random_state=42)\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    coherence_scores.append((k, coherence_score))\n",
        "\n",
        "best_k, best_coherence = max(coherence_scores, key=lambda x: x[1])\n",
        "\n",
        "# Train the LDA model with the best number of topics\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=best_k, random_state=42)\n",
        "\n",
        "# Print the topics\n",
        "print(f\"The best number of topics (K) based on coherence score: {best_k}\")\n",
        "print(\"Summarized Topics:\")\n",
        "for idx, topic in lda_model.print_topics():\n",
        "    print(f'Topic {idx + 1}: {topic}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "**Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bertopic\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlNS3rvvi5me",
        "outputId": "8fc9abe0-17b7-4d0f-e8e4-590c78762c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bertopic\n",
            "  Downloading bertopic-0.16.0-py2.py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.25.2)\n",
            "Collecting hdbscan>=0.8.29 (from bertopic)\n",
            "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn>=0.5.0 (from bertopic)\n",
            "  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.2)\n",
            "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.15.0)\n",
            "Collecting cython<3,>=0.27 (from hdbscan>=0.8.29->bertopic)\n",
            "  Using cached Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2023.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (24.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.38.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
            "  Downloading pynndescent-0.5.11-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (4.10.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m807.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Building wheels for collected packages: hdbscan, umap-learn\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp310-cp310-linux_x86_64.whl size=3039294 sha256=00cc41d7f6ccfa14f8e347c2550529d5e4791a273639745ca12d066a0a7a5ca6\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/0b/3b/dc4f60b7cc455efaefb62883a7483e76f09d06ca81cf87d610\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86832 sha256=a6a635c13f34700b5704c2b1d3192ae7d223232c6010cb6cfae98b208531594f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/70/07/428d2b58660a1a3b431db59b806a10da736612ebbc66c1bcc5\n",
            "Successfully built hdbscan umap-learn\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, cython, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pynndescent, nvidia-cusolver-cu12, hdbscan, umap-learn, sentence-transformers, bertopic\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.9\n",
            "    Uninstalling Cython-3.0.9:\n",
            "      Successfully uninstalled Cython-3.0.9\n",
            "Successfully installed bertopic-0.16.0 cython-0.29.37 hdbscan-0.8.33 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pynndescent-0.5.11 sentence-transformers-2.6.1 umap-learn-0.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "f9f04bd8-4e23-4593-dea1-ca46ffa461af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "k must be less than or equal to the number of training points",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-8ea5a854a4e0>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Fit BERTopic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Summarize the topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;31m# Cluster reduced embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cluster_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mumap_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;31m# Sort and Map Topic IDs by their frequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36m_cluster_embeddings\u001b[0;34m(self, umap_embeddings, documents, partial_fit, y)\u001b[0m\n\u001b[1;32m   3388\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3389\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3390\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhdbscan_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mumap_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3391\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3392\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhdbscan_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mumap_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hdbscan/hdbscan_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_prediction_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hdbscan/hdbscan_.py\u001b[0m in \u001b[0;36mgenerate_prediction_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             self._prediction_data = PredictionData(\n\u001b[0m\u001b[1;32m   1264\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcondensed_tree_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hdbscan/prediction.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, condensed_tree, min_samples, tree_type, metric, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         self.tree = self._tree_type_map[tree_type](self.raw_data,\n\u001b[1;32m    102\u001b[0m                                                    metric=metric, **kwargs)\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_distances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistanceMetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msklearn/neighbors/_binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors._kd_tree.BinaryTree.query\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: k must be less than or equal to the number of training points"
          ]
        }
      ],
      "source": [
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"One interesting text classification task related to cricket could be classifying match commentary or articles into different categories such as match summaries, player profiles, match predictions, and analysis pieces. Here are some features that could be useful for building a machine learning model for this task:\",\n",
        "    \"1. Word Frequency Features: The frequency of specific cricket-related terms or phrases in the text could be indicative of its category. For example, terms like 'century,' 'wicket,' 'runs,' 'dismissal,' and 'boundary' might be more common in match summaries, while terms like 'player statistics,' 'performance analysis,' and 'strategy' might be more common in analysis pieces.\",\n",
        "    \"2. N-grams: Analyzing sequences of words (n-grams) could provide valuable context. For instance, phrases like 'man of the match,' 'caught behind,' or 'run rate' might be indicative of specific types of content.\",\n",
        "    \"3. Sentiment Analysis: The sentiment expressed in the text could help classify articles as positive (e.g., celebrating a team's victory), negative (e.g., criticizing a player's performance), or neutral. This could be achieved by analyzing the sentiment of individual sentences or using pre-trained sentiment analysis models.\",\n",
        "    \"4. Named Entity Recognition (NER): Identifying named entities such as player names, team names, tournament names, and locations mentioned in the text could provide clues about the content's category. For example, mentions of players and teams are likely to appear in player profiles or match summaries.\",\n",
        "    \"5. Part-of-Speech (POS) Tagging: Analyzing the grammatical structure of the text by tagging words with their parts of speech could reveal patterns specific to certain categories. For instance, articles with a high frequency of verbs related to analysis (e.g., 'analyze,' 'evaluate,' 'assess') might belong to the analysis category.\",\n",
        "    \"6. Topic Modeling: Identifying underlying topics in the text using techniques like Latent Dirichlet Allocation (LDA) or Non-Negative Matrix Factorization (NMF) could help categorize articles based on the dominant themes present. For instance, topics related to match tactics, player performances, or match outcomes might be indicative of specific categories.\",\n",
        "    \"These features capture different aspects of the text content, allowing the machine learning model to learn patterns associated with each category and improve classification accuracy. By combining multiple types of features, the model can better understand the usage of cricket-related text and accurately classify it into relevant categories.\"\n",
        "]\n",
        "\n",
        "# Preprocess the text data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "texts = [' '.join([word for word in word_tokenize(document.lower()) if word.isalnum() and word not in stop_words]) for document in documents]\n",
        "\n",
        "# Initialize BERTopic model\n",
        "model = BERTopic()\n",
        "\n",
        "# Fit BERTopic\n",
        "topics, _ = model.fit_transform(texts)\n",
        "\n",
        "# Summarize the topics\n",
        "print(\"Summarized Topics:\")\n",
        "for topic_id in range(max(model.get_topic_freq().Topic)):\n",
        "    topic_words = model.get_topic(topic_id)[:10]  # Get top 10 words for each topic\n",
        "    print(f\"Topic {topic_id + 1}: {' '.join(topic_words)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Question (5 Points)\n",
        "\n",
        "**Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.**\n",
        "\n",
        "**This question will compensate for any points deducted in this exercise. Maximum marks for the exercise is 40 points.**"
      ],
      "metadata": {
        "id": "d89ODUx3jjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA (Latent Dirichlet Allocation):\n",
        "\n",
        "Pros: Provides interpretable topics, widely applicable in various contexts.\n",
        "Cons: Requires parameter tuning and is less effective with very large datasets.\n",
        "LSA (Latent Semantic Analysis):\n",
        "\n",
        "Pros: Offers consistent results, relatively easy to implement.\n",
        "Cons: Topics may lack interpretability and are sensitive to dataset size.\n",
        "Word Clouds:\n",
        "\n",
        "Pros: Simple to create and understand.\n",
        "Cons: Limited in capturing topic coherence and hierarchy, may lack depth in insights.\n",
        "BERTopic:\n",
        "\n",
        "Pros: Captures semantic relationships effectively and yields consistent outcomes.\n",
        "Cons: Demands greater computational resources and may struggle with scalability.\n"
      ],
      "metadata": {
        "id": "OK34nZtojhmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "Consider the following points in your response:\n",
        "\n",
        "**Learning Experience:** Describe your overall learning experience in working with text data and extracting features using various topic modeling algorithms. Did you understand these algorithms and did the implementations helped in grasping the nuances of feature extraction from text data.\n",
        "\n",
        "**Challenges Encountered:** Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It was a worthwhile learning experience to work with text data and extract features using topic modeling methods. My understanding of how to find latent subjects in the text to help with feature extraction has improved since I started using LDA. Preprocessing the text and adjusting the settings to get the best results were challenges. With its insights into textual data categorization and analysis—a critical skill in domains such as information retrieval, document classification, and sentiment analysis—this exercise is extremely pertinent to natural language processing (NLP).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}