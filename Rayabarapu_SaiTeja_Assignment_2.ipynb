{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RST0310/INFO-5731/blob/main/Rayabarapu_SaiTeja_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33c0439-3723-49ab-e3a7-0177b7860e81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests as rq\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "def get_movie_reviews(movie_id, max_reviews=1000):\n",
        "    reviews = []\n",
        "    url = f\"https://www.imdb.com/title/{movie_id}/reviews\"\n",
        "    review_count = 0\n",
        "    while review_count < max_reviews:\n",
        "        response = rq.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = Soup(response.text, 'html.parser')\n",
        "            reviews_data = soup.find_all('div', class_='text show-more__control')\n",
        "            reviews.extend([review.text.strip() for review in reviews_data])\n",
        "            review_count = len(reviews)\n",
        "            load_more_button = soup.find('div', class_='load-more-data')\n",
        "            if load_more_button:\n",
        "                load_more_url = f\"https://www.imdb.com{load_more_button['data-ajaxurl']}\"\n",
        "                response = rq.get(load_more_url)\n",
        "                if response.status_code == 200:\n",
        "                    time.sleep(2)  # Add a delay to avoid overwhelming the server\n",
        "                else:\n",
        "                    break  # Stop if unable to load more reviews\n",
        "            else:\n",
        "                break  # Stop if no more reviews to load\n",
        "        else:\n",
        "            break  # Stop if unable to fetch the initial page\n",
        "    return reviews[:max_reviews]\n",
        "\n",
        "def save_to_csv(reviews, file_name):\n",
        "    with open(file_name, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Review'])\n",
        "        for review in reviews:\n",
        "            writer.writerow([review])\n",
        "\n",
        "def fetch_movie_reviews(movie_id, file_name, max_reviews=1000):\n",
        "    reviews = get_movie_reviews(movie_id, max_reviews)\n",
        "    save_to_csv(reviews, file_name)\n",
        "\n",
        "movie_id = 'tt6791350'\n",
        "file_name = 'movie_reviews.csv'\n",
        "fetch_movie_reviews(movie_id, file_name, max_reviews=1000)\n"
      ],
      "metadata": {
        "id": "hLm0TTtsYDja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5610a5b2-647f-4ad0-e854-3e0520e98891"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data cleaning completed and saved to cleaned_movie_reviews.csv\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def remove_noise(text):\n",
        "    # Remove special characters and punctuation\n",
        "    cleaned_text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_numbers(text):\n",
        "    # Remove numbers\n",
        "    cleaned_text = ''.join([char for char in text if not char.isdigit()])\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "    return cleaned_text\n",
        "\n",
        "def lowercase(text):\n",
        "    # Convert text to lowercase\n",
        "    return text.lower()\n",
        "\n",
        "def stemming(text):\n",
        "    # Perform stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "    return stemmed_text\n",
        "\n",
        "def lemmatization(text):\n",
        "    # Perform lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    return lemmatized_text\n",
        "\n",
        "# Read the original CSV file containing the reviews\n",
        "original_file_name = 'movie_reviews.csv'\n",
        "cleaned_file_name = 'cleaned_movie_reviews.csv'\n",
        "\n",
        "with open(original_file_name, 'r', newline='', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f)\n",
        "    reviews = list(reader)\n",
        "\n",
        "# Remove header\n",
        "header = reviews[0]\n",
        "reviews = reviews[1:]\n",
        "\n",
        "# Choose either stemming or lemmatization\n",
        "use_stemming = True\n",
        "use_lemmatization = False\n",
        "\n",
        "# Apply cleaning steps to each review\n",
        "cleaned_reviews = []\n",
        "for review in reviews:\n",
        "    cleaned_review = review[:]  # Make a copy of the original review\n",
        "    text = review[0]  # Extract the review text from the row\n",
        "\n",
        "    # Step 1: Remove noise\n",
        "    text = remove_noise(text)\n",
        "\n",
        "    # Step 2: Remove numbers\n",
        "    text = remove_numbers(text)\n",
        "\n",
        "    # Step 3: Remove stopwords\n",
        "    text = remove_stopwords(text)\n",
        "\n",
        "    # Step 4: Lowercase\n",
        "    text = lowercase(text)\n",
        "\n",
        "    # Step 5: Apply stemming or lemmatization\n",
        "    if use_stemming:\n",
        "        text = stemming(text)\n",
        "    elif use_lemmatization:\n",
        "        text = lemmatization(text)\n",
        "\n",
        "    cleaned_review.append(text)  # Add cleaned text as a new column\n",
        "    cleaned_reviews.append(cleaned_review)\n",
        "\n",
        "# Write cleaned data to a new CSV file\n",
        "with open(cleaned_file_name, 'w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header + ['Cleaned Review'])  # Write header with new column name\n",
        "    writer.writerows(cleaned_reviews)\n",
        "\n",
        "print(\"Data cleaning completed and saved to\", cleaned_file_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "184d131e-6e9b-4537-be11-43c08e7fb0a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanfordnlp\n",
            "  Downloading stanfordnlp-0.2.0-py3-none-any.whl (158 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/158.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (1.25.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (2.31.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->stanfordnlp) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->stanfordnlp) (1.3.0)\n",
            "Installing collected packages: stanfordnlp\n",
            "Successfully installed stanfordnlp-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install stanfordnlp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanfordnlp\n",
        "stanfordnlp.download('en')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6VyQVVt4Zlf",
        "outputId": "0853b2c7-b62d-4b0c-a90d-c3c55fb2e1d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the default treebank \"en_ewt\" for language \"en\".\n",
            "Would you like to download the models for: en_ewt now? (Y/n)\n",
            "Y\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: en_ewt\n",
            "Download location: /root/stanfordnlp_resources/en_ewt_models.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 235M/235M [00:39<00:00, 5.92MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/en_ewt_models.zip\n",
            "Extracting models file for: en_ewt\n",
            "Cleaning up...Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from stanfordnlp.server import CoreNLPClient\n",
        "\n",
        "# Set the CoreNLP path\n",
        "corenlp_dir = '/root/stanfordnlp_resources'  # Update to the correct directory\n",
        "corenlp_zip = 'en_ewt_models.zip'  # Update to the correct zip file\n",
        "\n",
        "# Set CORENLP_HOME environment variable\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir\n",
        "\n",
        "# Start the server\n",
        "server = CoreNLPClient(\n",
        "    annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse'],\n",
        "    timeout=30000,\n",
        "    memory='4G',\n",
        "    endpoint='http://localhost:9000',\n",
        "    be_quiet=True,\n",
        "    corenlp_jars=[corenlp_dir + '/' + corenlp_zip])\n",
        "server.start()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGshkRBj5EYv",
        "outputId": "a3588682-33d8-4c64-ed85-7bed3bd030a9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting server with command: java -Xmx4G -cp /root/stanfordnlp_resources/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-69609c95f8d14a0f.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.parse import CoreNLPParser\n",
        "from nltk.tree import Tree\n",
        "import requests\n",
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('omw')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def is_corenlp_server_running():\n",
        "    try:\n",
        "        response = requests.get('http://localhost:9000')\n",
        "        return response.status_code == 200\n",
        "    except (requests.exceptions.ConnectionError, requests.exceptions.RequestException):\n",
        "        return False\n",
        "\n",
        "def pos_tagging(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens, tagset='universal')\n",
        "    return pos_tags\n",
        "\n",
        "def print_constituency_parsing_trees(text):\n",
        "    if not is_corenlp_server_running():\n",
        "        print(\"Error: CoreNLP server is not running at http://localhost:9000\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        parser = CoreNLPParser(url='http://localhost:9000', tagtype='pos')\n",
        "        sentences = sent_tokenize(text)\n",
        "        for sentence in sentences:\n",
        "            parsed_tree = list(parser.parse(sentence.split()))\n",
        "            if parsed_tree:\n",
        "                print(parsed_tree[0])\n",
        "            else:\n",
        "                print(\"Failed to parse constituency tree for sentence:\", sentence)\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "\n",
        "def print_dependency_parsing_trees(text):\n",
        "    if not is_corenlp_server_running():\n",
        "        print(\"Error: CoreNLP server is not running at http://localhost:9000\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        parser = CoreNLPParser(url='http://localhost:9000', tagtype='pos')\n",
        "        sentences = sent_tokenize(text)\n",
        "        for sentence in sentences:\n",
        "            parsed_tree = list(parser.parse(sentence.split()))\n",
        "            if parsed_tree:\n",
        "                dependency_tree = Tree.fromstring(parsed_tree[0].pretty_print())\n",
        "                print(dependency_tree)\n",
        "            else:\n",
        "                print(\"Failed to parse dependency tree for sentence:\", sentence)\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "\n",
        "def named_entity_recognition(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    entities = []\n",
        "    for sentence in sentences:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        tagged_tokens = nltk.pos_tag(tokens)\n",
        "        named_entities = nltk.ne_chunk(tagged_tokens)\n",
        "        for entity in named_entities:\n",
        "            if isinstance(entity, Tree):\n",
        "                entities.append(' '.join([leaf[0] for leaf in entity]))\n",
        "    return entities\n",
        "\n",
        "# Read the cleaned text from CSV\n",
        "cleaned_file_name = 'cleaned_movie_reviews.csv'\n",
        "cleaned_text = \"\"\n",
        "with open(cleaned_file_name, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        split_line = line.split(',')\n",
        "        if len(split_line) > 1:  # Check if split operation produces at least two elements\n",
        "            cleaned_text += split_line[1]  # Assuming the second column contains the cleaned text\n",
        "\n",
        "# (1) Parts of Speech (POS) Tagging\n",
        "if cleaned_text:\n",
        "    pos_tags = pos_tagging(cleaned_text)\n",
        "    noun_count = len([tag for _, tag in pos_tags if tag == 'NOUN'])\n",
        "    verb_count = len([tag for _, tag in pos_tags if tag == 'VERB'])\n",
        "    adj_count = len([tag for _, tag in pos_tags if tag == 'ADJ'])\n",
        "    adv_count = len([tag for _, tag in pos_tags if tag == 'ADV'])\n",
        "\n",
        "    print(\"(1) Parts of Speech (POS) Tagging:\")\n",
        "    print(\"Noun Count:\", noun_count)\n",
        "    print(\"Verb Count:\", verb_count)\n",
        "    print(\"Adjective Count:\", adj_count)\n",
        "    print(\"Adverb Count:\", adv_count)\n",
        "    print()\n",
        "else:\n",
        "    print(\"No text found for POS tagging.\")\n",
        "\n",
        "# (2) Constituency Parsing and Dependency Parsing\n",
        "if cleaned_text:\n",
        "    print(\"(2) Constituency Parsing Trees:\")\n",
        "    print_constituency_parsing_trees(cleaned_text)\n",
        "    print()\n",
        "\n",
        "    print(\"(3) Dependency Parsing Trees:\")\n",
        "    print_dependency_parsing_trees(cleaned_text)\n",
        "    print()\n",
        "else:\n",
        "    print(\"No text found for parsing.\")\n",
        "\n",
        "# (3) Named Entity Recognition\n",
        "if cleaned_text:\n",
        "    entities = named_entity_recognition(cleaned_text)\n",
        "    entity_counts = {}\n",
        "    for entity in entities:\n",
        "        entity_counts[entity] = entity_counts.get(entity, 0) + 1\n",
        "\n",
        "    print(\"(3) Named Entity Recognition:\")\n",
        "    for entity, count in entity_counts.items():\n",
        "        print(f\"{entity}: {count}\")\n",
        "else:\n",
        "    print(\"No text found for named entity recognition.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ee3bbMY6KIU",
        "outputId": "352201a6-0966-41be-ef2a-c5634544ab5b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]   Package omw is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1) Parts of Speech (POS) Tagging:\n",
            "Noun Count: 10522\n",
            "Verb Count: 5361\n",
            "Adjective Count: 3999\n",
            "Adverb Count: 2320\n",
            "\n",
            "(2) Constituency Parsing Trees:\n",
            "Error: CoreNLP server is not running at http://localhost:9000\n",
            "\n",
            "(3) Dependency Parsing Trees:\n",
            "Error: CoreNLP server is not running at http://localhost:9000\n",
            "\n",
            "(3) Named Entity Recognition:\n",
            "Review: 1\n",
            "Galaxy: 80\n",
            "MCU: 120\n",
            "Galaxy Vol: 80\n",
            "James: 40\n",
            "Gunn: 40\n",
            "Thanos: 40\n",
            "Kang: 80\n",
            "Quantumania: 40\n",
            "Adam: 80\n",
            "Adam Warlock: 40\n",
            "James Gunn: 80\n",
            "Rocket: 80\n",
            "GOTG: 40\n",
            "Marvel: 40\n",
            "Marvel Cinematic Universe: 40\n",
            "Vol: 40\n",
            "Disney: 40\n",
            "CGI: 40\n",
            "High: 40\n",
            "Guardians: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WziOwHv2WnZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The assignment is good enough to gain enough knowledge on webscraping from open sourses. The challenging thing was to get the error free code and get the desired out in the csv format. The provided time is really sufficient to complete the assignmnet. I was suprised to see the assignment opened up in the middle of the week and the deadline was on wednesday."
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}